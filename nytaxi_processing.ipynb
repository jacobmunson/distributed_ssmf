{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NY Taxi Data Cleaning Script\n",
    "\n",
    "### Building Tensor and Triplets, Multiple Data Files, Ride Type Not Combined\n",
    "\n",
    "Source data: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "I used these files:\n",
    "\n",
    "- yellow_tripdata_2020-03.parquet\n",
    "- yellow_tripdata_2020-04.parquet\n",
    "- green_tripdata_2020-03.parquet\n",
    "- green_tripdata_2020-04.parquet\n",
    "- fhv_tripdata_2020-03.parquet\n",
    "- fhv_tripdata_2020-04.parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import calendar\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "def build_tensor_and_triplets(\n",
    "    yellow_parquets:    List[Path],\n",
    "    green_parquets:     List[Path],\n",
    "    rideshare_parquets: List[Path],\n",
    "    year:               int,\n",
    "    start_month:        int,\n",
    "    end_month:          int,\n",
    "    out_prefix:         str = \"taxi_and_rideshare\"\n",
    ") -> Tuple[\n",
    "    pd.DataFrame,      # triplets_sparse\n",
    "    pd.DataFrame,      # triplets_full\n",
    "    np.ndarray,        # tensor X\n",
    "    pd.DatetimeIndex,  # time_index\n",
    "    pd.DataFrame       # location_map (loc_idx -> ride_type, origZoneID, composite_key)\n",
    "]:\n",
    "    \"\"\"\n",
    "    Build a sparse triplets table and a dense (PU × DO × Time) tensor, but treat\n",
    "    each (ride_type, zoneID) as its own unique location.\n",
    "    Produce a location_map to see which composite locations share the same original zoneID across types.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yellow_parquets : List[Path]\n",
    "        Paths to yellow‐taxi Parquet files (e.g. [\"yellow_tripdata_2020-01.parquet\", ...]).\n",
    "        Each must contain at least: \"PULocationID\" (int/float) and \"DOLocationID\", plus\n",
    "        a timestamp column named \"tpep_pickup_datetime\".\n",
    "    green_parquets : List[Path]\n",
    "        Paths to green‐taxi Parquet files. Each must contain \"PULocationID\", \"DOLocationID\",\n",
    "        and a timestamp column \"lpep_pickup_datetime\".\n",
    "    rideshare_parquets : List[Path]\n",
    "        Paths to rideshare (FHV) Parquet files. Each must contain \"PUlocationID\" (note\n",
    "        the lowercase \"l\"), \"DOlocationID\", and a timestamp column \"pickup_datetime\".\n",
    "    year : int\n",
    "        Four‐digit year, e.g. 2020.\n",
    "    start_month : int\n",
    "        Starting month (1=Jan … 12=Dec). Must satisfy 1 <= start_month <= end_month <= 12.\n",
    "    end_month : int\n",
    "        Ending month (1=Jan … 12=Dec). Must satisfy start_month <= end_month <= 12.\n",
    "    out_prefix : str, default \"taxi_and_rideshare\"\n",
    "        Prefix for writing out:\n",
    "          - {out_prefix}_triplets.parquet\n",
    "          - {out_prefix}_triplets_zeroes.parquet\n",
    "          - {out_prefix}_tensor.npy\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    triplets_sparse : pd.DataFrame\n",
    "        DataFrame with columns [PU_idx, DO_idx, t_idx, trip_count] for all positive counts,\n",
    "        where PU_idx and DO_idx refer to the NEW composite‐location indices (0..N−1).\n",
    "    triplets_full : pd.DataFrame\n",
    "        Same as \"triplets_sparse\" but with EVERY combination (PU_idx, DO_idx, t_idx) filled in (zeros where no trips).\n",
    "    X : np.ndarray\n",
    "        Dense NumPy array of shape (N, N, T), where N = total number of unique\n",
    "        (ride_type × zoneID) composites, and T = # of hours from (start_month, start_day=1)\n",
    "        through the last hour of (end_month, last_day) at 23:00.  \n",
    "        Entry X[i,j,k] = number of trips from composite_loc i -> composite_loc j during hour‐index k.\n",
    "    time_index : pd.DatetimeIndex\n",
    "        The hourly index from year‐start_month‐01 00:00 through year‐end_month‐(last day) 23:00.\n",
    "    location_map : pd.DataFrame\n",
    "        A DataFrame of shape (N, 3) with columns:\n",
    "          • loc_idx         (int, 0..N−1)  \n",
    "          • ride_type       (string: \"yellow\" | \"green\" | \"rideshare\")  \n",
    "          • orig_zone_id    (int)  (the original numeric zone ID)  \n",
    "          • composite_key   (string: e.g. \"yellow_264\", \"rideshare_  5\", …)  \n",
    "\n",
    "        Multiple rows in \"location_map\" with the same \"orig_zone_id\"\n",
    "        but different \"ride_type\", then composite locations refer to the same\n",
    "        underlying zone.  \n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If start_month/end_month are invalid, or if no rows remain after filtering.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Validate months\n",
    "    if not (1 <= start_month <= 12 and 1 <= end_month <= 12 and start_month <= end_month):\n",
    "        raise ValueError(\"start_month and end_month must be in 1..12, and start_month ≤ end_month.\")\n",
    "\n",
    "    # 2) Compute the absolute start/end timestamps for filtering\n",
    "    start_ts = pd.Timestamp(year, start_month, 1, 0, 0)\n",
    "    last_day_end = calendar.monthrange(year, end_month)[1]\n",
    "    end_ts = pd.Timestamp(year, end_month, last_day_end, 23, 59)\n",
    "\n",
    "    # 3) Helper to load & filter one \"category\" (yellow/green/rideshare).\n",
    "    #    It also renames FHV's lowercase‐l columns to match taxi's uppercase‐\"L\".\n",
    "    def _load_and_filter(\n",
    "        paths: List[Path],\n",
    "        time_col: str,\n",
    "        ride_type: str,\n",
    "        rename_map: dict = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        For each parquet in 'paths', read it, optionally rename columns, parse 'time_col',\n",
    "        drop rows with missing PU/DO, filter to [start_ts..end_ts], floor to the hour,\n",
    "        and return a concatenated DataFrame with columns:\n",
    "          [ride_type, PULocationID, DOLocationID, time_hour]\n",
    "        \"\"\"\n",
    "        pieces = []\n",
    "        for pq_path in paths:\n",
    "            df = pd.read_parquet(pq_path).copy()\n",
    "\n",
    "            # (a) If we need to rename (FHV uses lowercase-l), do so now:\n",
    "            if rename_map:\n",
    "                df = df.rename(columns=rename_map)\n",
    "\n",
    "            # (b) Parse the timestamp:\n",
    "            df[\"time\"] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
    "\n",
    "            # (c) Drop any row where PU or DO is NaN\n",
    "            df = df.dropna(subset=[\"PULocationID\", \"DOLocationID\"])\n",
    "\n",
    "            # (d) Keep only rows within [start_ts .. end_ts]\n",
    "            df = df[(df[\"time\"] >= start_ts) & (df[\"time\"] <= end_ts)]\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            # (e) Floor to the hour\n",
    "            df[\"time_hour\"] = df[\"time\"].dt.floor(\"H\")\n",
    "\n",
    "            # (f) Keep exactly these columns—and add a literal 'ride_type':\n",
    "            df = df[[\"PULocationID\", \"DOLocationID\", \"time_hour\"]].copy()\n",
    "            df[\"ride_type\"] = ride_type\n",
    "\n",
    "            pieces.append(df)\n",
    "\n",
    "        if not pieces:\n",
    "            # If no file yielded any rows, return an empty DF with the right columns\n",
    "            return pd.DataFrame(columns=[\"PULocationID\", \"DOLocationID\", \"time_hour\", \"ride_type\"])\n",
    "        return pd.concat(pieces, ignore_index=True)\n",
    "\n",
    "    # 4) Load each category:\n",
    "    df_yellow = _load_and_filter(\n",
    "        yellow_parquets,\n",
    "        time_col=\"tpep_pickup_datetime\",\n",
    "        ride_type=\"yellow\",\n",
    "        rename_map=None\n",
    "    )\n",
    "    df_green = _load_and_filter(\n",
    "        green_parquets,\n",
    "        time_col=\"lpep_pickup_datetime\",\n",
    "        ride_type=\"green\",\n",
    "        rename_map=None\n",
    "    )\n",
    "    df_rideshare = _load_and_filter(\n",
    "        rideshare_parquets,\n",
    "        time_col=\"pickup_datetime\",\n",
    "        ride_type=\"rideshare\",\n",
    "        # rename the lowercase‐l columns to uppercase‐L so they match \"PULocationID\" etc.\n",
    "        rename_map={\"PUlocationID\": \"PULocationID\", \"DOlocationID\": \"DOLocationID\"}\n",
    "    )\n",
    "\n",
    "    # 5) Concatenate all three types into one big DataFrame\n",
    "    df_all = pd.concat([df_yellow, df_green, df_rideshare], ignore_index=True)\n",
    "    if df_all.empty:\n",
    "        raise ValueError(f\"No rows found between {start_ts} and {end_ts} in any file.\")\n",
    "\n",
    "    # 6) Build a \"composite key\" for each location = f\"{ride_type}_{origZoneID}\".\n",
    "    #    (We'll do this for both pickup and dropoff columns.)\n",
    "    df_all[\"PU_composite\"] = (\n",
    "        df_all[\"ride_type\"] + \"_\" + df_all[\"PULocationID\"].astype(int).astype(str)\n",
    "    )\n",
    "    df_all[\"DO_composite\"] = (\n",
    "        df_all[\"ride_type\"] + \"_\" + df_all[\"DOLocationID\"].astype(int).astype(str)\n",
    "    )\n",
    "\n",
    "    # 7) Gather all unique composite‐keys (pickup ∪ dropoff)\n",
    "    pu_uniques = df_all[\"PU_composite\"].unique()\n",
    "    do_uniques = df_all[\"DO_composite\"].unique()\n",
    "    all_composites = np.unique(np.concatenate([pu_uniques, do_uniques])).tolist()\n",
    "\n",
    "    # 8) Create a lookup: composite_key -> loc_idx (0 .. N−1)\n",
    "    #    (We sort so that it's deterministic, but any ordering is fine.)\n",
    "    all_composites.sort()\n",
    "    composite2idx = {key: idx for idx, key in enumerate(all_composites)}\n",
    "    N = len(all_composites)\n",
    "    print(f\"Found {N} distinct (ride_type × zone) composites.\")\n",
    "\n",
    "    # 9) Build a \"location_map\" DataFrame so you know:\n",
    "    #    loc_idx | ride_type | orig_zone_id | composite_key\n",
    "    #    ---------------------------------------------------\n",
    "    #    0       | \"green\"   |  10          | \"green_10\"\n",
    "    #    1       | \"rideshare\"| 10          | \"rideshare_10\"\n",
    "    #    2       | \"yellow\"  | 264          | \"yellow_264\"\n",
    "    #    … etc …\n",
    "    #\n",
    "    loc_rows = []\n",
    "    for composite_key, loc_idx in composite2idx.items():\n",
    "        ride, orig_str = composite_key.split(\"_\", 1)\n",
    "        loc_rows.append({\n",
    "            \"loc_idx\":      loc_idx,\n",
    "            \"ride_type\":    ride,\n",
    "            \"orig_zone_id\": int(orig_str),\n",
    "            \"composite_key\": composite_key\n",
    "        })\n",
    "    location_map = pd.DataFrame(loc_rows).sort_values(\"loc_idx\").reset_index(drop=True)\n",
    "\n",
    "    # 10) Now map every row in df_all to two integer indices:\n",
    "    #     PU_idx_local = composite2idx[ PU_composite ]\n",
    "    #     DO_idx_local = composite2idx[ DO_composite ]\n",
    "    df_all[\"PU_idx\"] = df_all[\"PU_composite\"].map(composite2idx).astype(int)\n",
    "    df_all[\"DO_idx\"] = df_all[\"DO_composite\"].map(composite2idx).astype(int)\n",
    "\n",
    "    # 11) Build the hourly time_index from start_ts (00:00) through the last full hour\n",
    "    #     before end_ts. E.g. if end_ts = 2020-04-30 23:59, last hour = 2020-04-30 23:00.\n",
    "    last_hour = pd.Timestamp(year, end_month, last_day_end, 23, 0)\n",
    "    time_index = pd.date_range(start=start_ts, end=last_hour, freq=\"H\")\n",
    "    T = len(time_index)\n",
    "    print(\"Total hours T =\", T)\n",
    "\n",
    "    # 12) Create a mapping for time -> index\n",
    "    T2idx = {ts: idx for idx, ts in enumerate(time_index)}\n",
    "\n",
    "    # 13) Map \"time_hour\" -> integer hour‐index\n",
    "    df_all[\"t_idx\"] = df_all[\"time_hour\"].map(T2idx).astype(int)\n",
    "\n",
    "    # 14) Build sparse triplets by grouping on (PU_idx, DO_idx, t_idx)\n",
    "    triplets_sparse = (\n",
    "        df_all\n",
    "        .assign(trip_count=1)\n",
    "        .groupby([\"PU_idx\", \"DO_idx\", \"t_idx\"], sort=False)[\"trip_count\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "    sparse_path = f\"{out_prefix}_triplets.parquet\"\n",
    "    triplets_sparse.to_parquet(sparse_path, index=False)\n",
    "    print(\"DONE - Saved sparse triplets ->\", sparse_path)\n",
    "\n",
    "    # 15) Build the \"full\" cartesian product of (PU_idx, DO_idx, t_idx), filling zeros\n",
    "    full_idx = pd.MultiIndex.from_product(\n",
    "        [range(N), range(N), range(T)],\n",
    "        names=[\"PU_idx\", \"DO_idx\", \"t_idx\"]\n",
    "    )\n",
    "    triplets_full = (\n",
    "        triplets_sparse\n",
    "        .set_index([\"PU_idx\", \"DO_idx\", \"t_idx\"])\n",
    "        .reindex(full_idx, fill_value=0)\n",
    "        .reset_index()\n",
    "    )\n",
    "    full_path = f\"{out_prefix}_triplets_zeroes.parquet\"\n",
    "    triplets_full.to_parquet(full_path, index=False)\n",
    "    print(\"DONE - Saved full triplets ->\", full_path)\n",
    "\n",
    "    # 16) Build the dense tensor X of shape (N, N, T)\n",
    "    X = np.zeros((N, N, T), dtype=int)\n",
    "    X[\n",
    "        triplets_full[\"PU_idx\"].values,\n",
    "        triplets_full[\"DO_idx\"].values,\n",
    "        triplets_full[\"t_idx\"].values\n",
    "    ] = triplets_full[\"trip_count\"].values\n",
    "\n",
    "    tensor_path = f\"{out_prefix}_tensor.npy\"\n",
    "    np.save(tensor_path, X)\n",
    "    print(\"DONE - Saved tensor ->\", tensor_path, \"shape=\", X.shape)\n",
    "\n",
    "    # SANITY CHECKS\n",
    "    assert X.shape == (N, N, T), f\"Tensor shape mismatch: {X.shape} vs ({N},{N},{T})\"\n",
    "    nnz = np.count_nonzero(X)\n",
    "    assert len(triplets_sparse) == nnz, f\"FAIL - sparse count {len(triplets_sparse)} vs nonzero {nnz}\"\n",
    "    expected_full = N * N * T\n",
    "    assert len(triplets_full) == expected_full, (\n",
    "        f\"FAIL - full count {len(triplets_full)} vs expected {expected_full}\"\n",
    "    )\n",
    "    # Check that rebuilding from triplets_full gives X exactly\n",
    "    Y_full = np.zeros_like(X)\n",
    "    Y_full[\n",
    "        triplets_full[\"PU_idx\"].values,\n",
    "        triplets_full[\"DO_idx\"].values,\n",
    "        triplets_full[\"t_idx\"].values\n",
    "    ] = triplets_full[\"trip_count\"].values\n",
    "    assert np.array_equal(Y_full, X), \"FAIL - full table =/= tensor\"\n",
    "\n",
    "    print(\"SUCCESS - All files consistent and correct.\")\n",
    "\n",
    "    return triplets_sparse, triplets_full, X, time_index, location_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map.to_csv(\"taxi_yellow_green_rideshare_distinct_march_to_apr2020_locations_map.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_files = [\n",
    "    Path(\"Raw Data/yellow_tripdata_2020-03.parquet\"),\n",
    "    Path(\"Raw Data/yellow_tripdata_2020-04.parquet\"),\n",
    "]\n",
    "green_files  = [\n",
    "    Path(\"Raw Data/green_tripdata_2020-03.parquet\"),\n",
    "    Path(\"Raw Data/green_tripdata_2020-04.parquet\"),\n",
    "]\n",
    "rideshare_files  = [\n",
    "    Path(\"Raw Data/fhv_tripdata_2020-03.parquet\"),\n",
    "    Path(\"Raw Data/fhv_tripdata_2020-04.parquet\"),\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_df, full_df, tensor, time_index, location_map = build_tensor_and_triplets(\n",
    "    yellow_parquets    = yellow_files,\n",
    "    green_parquets     = green_files,\n",
    "    rideshare_parquets = rideshare_files,\n",
    "    year               = 2020,\n",
    "    start_month        = 3,\n",
    "    end_month          = 4,\n",
    "    out_prefix         = \"taxi_yellow_green_rideshare_distinct_march_to_apr2020\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DistributedSSMF_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
